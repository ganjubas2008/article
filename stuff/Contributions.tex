\subsection{Assumptions}
\begin{assumption} \label{ass:ass_1}
    Assume that $F$ is $\mu$-strongly convex and $L$-smooth. That is, $\forall x, y \in \mathbb{R}^d$,
    \[
    \frac{\mu}{2} \norm{x - y}^2 \leq F(y) - F(x) - \inner{\nabla F(x)}{y-x} \leq \frac{L}{2} \norm {x-y}^2
    \]
\end{assumption}

\begin{corollary} Under assumption~\ref{ass:ass_1}
    \[
    \frac{1}{2L} \norm{\nabla F(x) - \nabla F(y)} \leq F(y) - F(x) - \inner{\nabla F(x)}{y-x}
    \]
\end{corollary}
\proof{
    This is Theorem 2.1.5 in \cite{Nesterov}
}

\begin{assumption} \label{ass:ass_2}
    Assume that for stochastic gradience variance is bounded, i.e. exists such $\sigma$ that:
    \[\E_{z \sim \mathcal{D}} \norm {\nabla F(x, z) - \nabla F(x)}^2 \leq \sigma^2
    + \rho \norm{\nabla F(x)}^2 \]
\end{assumption}

\begin{assumption} \label{ass:ass_3}
    Let $F = Q + R$, where $Q$ is a quadratic function that is $\mu_Q$-convex and $L_Q$-smooth, and $R$ is $\mu_R$-convex and $L_R$-smooth. Than we denote parameter
    \(\varepsilon := \frac{L_R}{L}\)
    which gives us an idea of how far $F$ is from a quadratic function.
\end{assumption}

Under assumption~\ref{ass:ass_3}, following three statements take place:
\begin{corollary} \label{cor:linearity}
    $\nabla Q$ is a linear function
\end{corollary}

\begin{corollary}
    \(\varepsilon \leq 1\)
\end{corollary}

\begin{corollary}
    \(\mu_Q + \mu_R \leq \mu\)
\end{corollary}

\input{stuff/LocalSgd}

\subsection{Related Work}
Under assumptions~\ref{ass:ass_1} and~\ref{ass:ass_2}, Khaled et al. \cite{Khaled} prove that for $\mu > 0$
\begin{equation}
\E \norm{\bar{x}_T - x_*}^2
\leq
(1 - \gamma \mu)^T \norm{x_0 - x_*}^2 
+ \frac{\gamma \sigma^2}{\mu M} 
+ \frac{2 \gamma^2 \sigma^2 L (H-1)}{\mu}
\end{equation}

and for $\mu = 0$:

\begin{equation}
\E[F(\bar{x}_t - x_*] \leq \frac{2}{\gamma T} + \frac{2 \gamma \sigma^2}{M} + 4 \gamma^2 L \sigma^2 (H-1)
\end{equation}

\subsection{Our contributions}
In this work, we mainly focus on the improvement in the last term of both convergence rates provided by Khaled et al. under additional assumption~\ref{ass:ass_3}.

\begin{theorem} \label{th:th_1}
    Adding assumption~\ref{ass:ass_3} to~\ref{ass:ass_1} and~\ref{ass:ass_2}, 
    considering $\mu > 0$ and $\gamma \leq \frac{1}{6 L}$ gives:
    \begin{align}
        \E \norm{\bar{x}_T - x_*}^2
        \leq
        (1 - \gamma \mu)^T \norm{x_0 - x_*}^2 
        + \frac{\gamma \sigma^2}{\mu M} 
        + \frac{10 \varepsilon \gamma^2 \sigma^2 L (H-1)}{\mu}
    \end{align}
\end{theorem}

\begin{corollary} \label{}
    As it was previously shown in \cite{Woodworth}, an important special case is is achieved when epsilon is equal to zero. Than from the Theorem~\ref{th:th_1} it follows that:
    \begin{align}
        \E[\norm{\bar{x}_T - x_*}^2]
        \leq
        (1 - \gamma \mu)^T \norm{x_0 - x_*}^2 
        + \frac{\gamma \sigma^2}{\mu M}
    \end{align}
\end{corollary}

Thus, if $F$ is a quadratic function, the upper bound on the rate of convergence is independent of the communication frequency.

\begin{theorem} \label{th:th_2}
    Adding assumption~\ref{ass:ass_3} to~\ref{ass:ass_1} and~\ref{ass:ass_1}, 
    considering $\mu = 0$ and $\gamma \leq \frac{1}{6 L}$
    \begin{align}
        \E[F(\hat{x}_t) - x_*] \leq
        \frac{2}{\gamma T} + \frac{2 \gamma \sigma^2}{M} + 20 \varepsilon \gamma^2 L \sigma^2 (H-1)
    \end{align}
\end{theorem}