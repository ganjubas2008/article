
%Convexity and nabla-Lipschitzness
\begin{assumption} \label{ass:ass_1}
    Assume that $F$ is $\mu$-convex and $L$-smooth. That is, $\forall x, y \in \mathbb{R}^d$,
    \[
    \frac{\mu}{2} \norm{x - y}^2 \leq F(y) - F(x) - \inner{\nabla F(x)}{y-x} \leq \frac{L}{2} \norm {x-y}^2
    \]
\end{assumption}

\begin{corollary} \label{cor:nesterov}
Under assumption~\ref{ass:ass_1}
    \[
    \frac{1}{2L} \norm{\nabla F(x) - \nabla F(y)}^2 \leq F(y) - F(x) - \inner{\nabla F(x)}{y-x}
    \]
\end{corollary}
\proof{
    This is Theorem 2.1.5 in \cite{Nesterov}
}


%Bounded stochastic gradient variance
\begin{assumption} \label{ass:ass_2}
    Exist such constants $\sigma$ and $\rho$ that:
    \[\E_{z \sim \mathcal{D}} \norm {\nabla F(x, z) - \nabla F(x)}^2 \leq \sigma^2
    + \rho \norm{\nabla F(x)}^2 \]
\end{assumption}

\begin{assumption} \label{ass:ass_3}
    The stochastic gradient is an unbiased estimate of its expectation:
    \[ \E_{z \sim \mathcal{D}}[\nabla(F(x, z)] = F(x)\]
\end{assumption}

%F=Q+R decomposition
\begin{statement}[\varepsilon - decomposition] \label{st:st_1}
    Objective function can be decomposed as $F = Q + R$, where $Q$ is a quadratic function that is $\mu_Q$-convex and $L_Q$-smooth, and $R$ is $\mu_R$-convex and $L_R$-smooth. Than we denote parameter
    \(\varepsilon := \frac{L_R}{L}\)
    which gives us an idea of how far $F$ is from a quadratic function.
\end{statement}

Note that such decomposition always takes place because we can take $Q = 0$ and $R = F$.

We will denote the sum of $\mu_Q$ and $\mu_R$ as $\lambda$, and $\frac{L}{\lambda}$ as $\kappa$. 

\begin{corollary} \label{cor:linearity}
    In the prospect of Statement~\ref{st:st_1}, following takes place: $\,$
    \begin{enumerate}
    \item[a)] $\nabla Q$ is a linear function
    \item[b)] \(\varepsilon \leq 1\)

    \item[c)] \(\lambda \leq \mu\)
\end{enumerate}
\end{corollary}