@misc{Khaled,
      title={Tighter Theory for Local SGD on Identical and Heterogeneous Data}, 
      author={Ahmed Khaled and Konstantin Mishchenko and Peter Richtárik},
      year={2022},
      eprint={1909.04746},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{Woodworth,
      title={Is Local SGD Better than Minibatch SGD?}, 
      author={Blake Woodworth and Kumar Kshitij Patel and Sebastian U. Stich and Zhen Dai and Brian Bullins and H. Brendan McMahan and Ohad Shamir and Nathan Srebro},
      year={2020},
      eprint={2002.07839},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{Stich,
      title={Local SGD Converges Fast and Communicates Little}, 
      author={Sebastian U. Stich},
      year={2019},
      eprint={1805.09767},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{Stich_stepsize,
  author       = {Sebastian U. Stich},
  title        = {Unified Optimal Analysis of the (Stochastic) Gradient Method},
  journal      = {CoRR},
  volume       = {abs/1907.04232},
  year         = {2019},
  url          = {http://arxiv.org/abs/1907.04232},
  eprinttype    = {arXiv},
  eprint       = {1907.04232},
  timestamp    = {Wed, 17 Jul 2019 10:27:35 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1907-04232.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@book{Nesterov,
    title = {Introductory Lectures on Convex Optimization: A Basic Course},
    author = {Nesterov, Yurii},
    year = {2014},
    isbn = {1461346916},
    publisher = {Springer Publishing Company, Incorporated},
    edition = {1}
}

@article{FedAC,
  author       = {Honglin Yuan and
                  Tengyu Ma},
  title        = {Federated Accelerated Stochastic Gradient Descent},
  journal      = {CoRR},
  volume       = {abs/2006.08950},
  year         = {2020},
  url          = {https://arxiv.org/abs/2006.08950},
  eprinttype    = {arXiv},
  eprint       = {2006.08950},
  timestamp    = {Sun, 08 Aug 2021 16:40:33 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2006-08950.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Spiridonoff,
  author       = {Artin Spiridonoff and
                  Alex Olshevsky and
                  Ioannis Ch. Paschalidis},
  title        = {Communication-efficient {SGD:} From Local {SGD} to One-Shot Averaging},
  journal      = {CoRR},
  volume       = {abs/2106.04759},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.04759},
  eprinttype    = {arXiv},
  eprint       = {2106.04759},
  timestamp    = {Tue, 15 Jun 2021 16:35:15 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-04759.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Scaffold,
  author       = {Sai Praneeth Karimireddy and
                  Satyen Kale and
                  Mehryar Mohri and
                  Sashank J. Reddi and
                  Sebastian U. Stich and
                  Ananda Theertha Suresh},
  title        = {{SCAFFOLD:} Stochastic Controlled Averaging for On-Device Federated
                  Learning},
  journal      = {CoRR},
  volume       = {abs/1910.06378},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.06378},
  eprinttype    = {arXiv},
  eprint       = {1910.06378},
  timestamp    = {Wed, 16 Oct 2019 16:25:53 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-06378.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}







@article{Mangasarian,
author = {Mangasarian, L. O.},
title = {Parallel Gradient Distribution in Unconstrained Optimization},
journal = {SIAM Journal on Control and Optimization},
volume = {33},
number = {6},
pages = {1916-1925},
year = {1995},
doi = {10.1137/S0363012993250220},

URL = { 
    
        https://doi.org/10.1137/S0363012993250220
    
    

},
eprint = { 
    
        https://doi.org/10.1137/S0363012993250220
    
    

}
,
    abstract = { A parallel version is proposed for a fundamental theorem of serial unconstrained optimization. The parallel theorem allows each of k parallel processors to use simultaneously a different algorithm, such as a descent, Newton, quasi-Newton, or conjugate gradient algorithm. Each processor can perform one or many steps of a serial algorithm on a portion of the gradient of the objective function assigned to it, independently of the other processors. Eventually a synchronization step is performed which, for differentiable convex functions, consists of taking a strong convex combination of the k points found by the k processors. A more general synchronization step, applicable to convex as well as nonconvex functions, consists of taking the best point found by the k processors or any point that is better. The fundamental result that we establish is that any accumulation point of the parallel algorithm is stationary for the nonconvex case and is a global solution for the convex case. Computational testing on the Thinking Machines CM-5 multiprocessor indicates a speedup of the order of the number of processors employed. }
}

@misc{LLaMA,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Hospitals,
title = {Federated Learning in Medical Imaging: Part II: Methods, Challenges, and Considerations},
journal = {Journal of the American College of Radiology},
volume = {19},
number = {8},
pages = {975-982},
year = {2022},
issn = {1546-1440},
doi = {https://doi.org/10.1016/j.jacr.2022.03.016},
url = {https://www.sciencedirect.com/science/article/pii/S1546144022002812},
author = {Erfan Darzidehkalani and Mohammad Ghasemi-rad and P.M.A. {van Ooijen}},
keywords = {Federated learning, medical imaging, privacy-preserving machine learning},
abstract = {Federated learning is a machine learning method that allows decentralized training of deep neural networks among multiple clients while preserving the privacy of each client’s data. Federated learning is instrumental in medical imaging because of the privacy considerations of medical data. Setting up federated networks in hospitals comes with unique challenges, primarily because medical imaging data and federated learning algorithms each have their own set of distinct characteristics. This article introduces federated learning algorithms in medical imaging and discusses technical challenges and considerations of real-world implementation of them.}
}

@inproceedings{DivPatel3rdOrderBound,
 author = {Dieuleveut, Aymeric and Patel, Kumar Kshitij},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Communication trade-offs for Local-SGD with large step size},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/4aadd661908b181d059a117f02fbc9ec-Paper.pdf},
 volume = {32},
 year = {2019}
}

@misc{Hetero<Id,
      title={Measuring the Effects of Non-Identical Data Distribution for Federated Visual Classification}, 
      author={Tzu-Ming Harry Hsu and Hang Qi and Matthew Brown},
      year={2019},
      eprint={1909.06335},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{Li,
      title={On the Convergence of FedAvg on Non-IID Data}, 
      author={Xiang Li and Kaixuan Huang and Wenhao Yang and Shusen Wang and Zhihua Zhang},
      year={2020},
      eprint={1907.02189},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{LocalSGD2024,
      title={Asynchronous Local-SGD Training for Language Modeling}, 
      author={Bo Liu and Rachita Chhaparia and Arthur Douillard and Satyen Kale and Andrei A. Rusu and Jiajun Shen and Arthur Szlam and Marc'Aurelio Ranzato},
      year={2024},
      eprint={2401.09135},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{Gemini,
      title={Gemini: A Family of Highly Capable Multimodal Models}, 
      author={Rohan Anil et al.}
      year={2024},
      eprint={2312.11805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{LowerBound,
      title={Sharp Bounds for Federated Averaging (Local SGD) and Continuous Perspective}, 
      author={Margalit Glasgow and Honglin Yuan and Tengyu Ma},
      year={2022},
      eprint={2111.03741},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@InProceedings{McMahan,
  title = 	 {{Communication-Efficient Learning of Deep Networks from Decentralized Data}},
  author = 	 {McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and Arcas, Blaise Aguera y},
  booktitle = 	 {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1273--1282},
  year = 	 {2017},
  editor = 	 {Singh, Aarti and Zhu, Jerry},
  volume = 	 {54},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {20--22 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v54/mcmahan17a/mcmahan17a.pdf},
  url = 	 {https://proceedings.mlr.press/v54/mcmahan17a.html},
  abstract = 	 {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches.  We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.  We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent. }
}
