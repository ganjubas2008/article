@misc{Khaled,
      title={Tighter Theory for Local SGD on Identical and Heterogeneous Data}, 
      author={Ahmed Khaled and Konstantin Mishchenko and Peter Richtárik},
      year={2022},
      eprint={1909.04746},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{Woodworth,
      title={Is Local SGD Better than Minibatch SGD?}, 
      author={Blake Woodworth and Kumar Kshitij Patel and Sebastian U. Stich and Zhen Dai and Brian Bullins and H. Brendan McMahan and Ohad Shamir and Nathan Srebro},
      year={2020},
      eprint={2002.07839},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{Stich,
      title={Local SGD Converges Fast and Communicates Little}, 
      author={Sebastian U. Stich},
      year={2019},
      eprint={1805.09767},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@article{Stich_stepsize,
  author       = {Sebastian U. Stich},
  title        = {Unified Optimal Analysis of the (Stochastic) Gradient Method},
  journal      = {CoRR},
  volume       = {abs/1907.04232},
  year         = {2019},
  url          = {http://arxiv.org/abs/1907.04232},
  eprinttype    = {arXiv},
  eprint       = {1907.04232},
  timestamp    = {Wed, 17 Jul 2019 10:27:35 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1907-04232.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@book{Nesterov,
    title = {Introductory Lectures on Convex Optimization: A Basic Course},
    author = {Nesterov, Yurii},
    year = {2014},
    isbn = {1461346916},
    publisher = {Springer Publishing Company, Incorporated},
    edition = {1}
}

@article{FedAC,
  author       = {Honglin Yuan and
                  Tengyu Ma},
  title        = {Federated Accelerated Stochastic Gradient Descent},
  journal      = {CoRR},
  volume       = {abs/2006.08950},
  year         = {2020},
  url          = {https://arxiv.org/abs/2006.08950},
  eprinttype    = {arXiv},
  eprint       = {2006.08950},
  timestamp    = {Sun, 08 Aug 2021 16:40:33 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2006-08950.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Spiridonoff,
  author       = {Artin Spiridonoff and
                  Alex Olshevsky and
                  Ioannis Ch. Paschalidis},
  title        = {Communication-efficient {SGD:} From Local {SGD} to One-Shot Averaging},
  journal      = {CoRR},
  volume       = {abs/2106.04759},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.04759},
  eprinttype    = {arXiv},
  eprint       = {2106.04759},
  timestamp    = {Tue, 15 Jun 2021 16:35:15 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-04759.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Scaffold,
  author       = {Sai Praneeth Karimireddy and
                  Satyen Kale and
                  Mehryar Mohri and
                  Sashank J. Reddi and
                  Sebastian U. Stich and
                  Ananda Theertha Suresh},
  title        = {{SCAFFOLD:} Stochastic Controlled Averaging for On-Device Federated
                  Learning},
  journal      = {CoRR},
  volume       = {abs/1910.06378},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.06378},
  eprinttype    = {arXiv},
  eprint       = {1910.06378},
  timestamp    = {Wed, 16 Oct 2019 16:25:53 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-06378.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}







@article{Mangasarian,
author = {Mangasarian, L. O.},
title = {Parallel Gradient Distribution in Unconstrained Optimization},
journal = {SIAM Journal on Control and Optimization},
volume = {33},
number = {6},
pages = {1916-1925},
year = {1995},
doi = {10.1137/S0363012993250220},

URL = { 
    
        https://doi.org/10.1137/S0363012993250220
    
    

},
eprint = { 
    
        https://doi.org/10.1137/S0363012993250220
    
    

}
,
    abstract = { A parallel version is proposed for a fundamental theorem of serial unconstrained optimization. The parallel theorem allows each of k parallel processors to use simultaneously a different algorithm, such as a descent, Newton, quasi-Newton, or conjugate gradient algorithm. Each processor can perform one or many steps of a serial algorithm on a portion of the gradient of the objective function assigned to it, independently of the other processors. Eventually a synchronization step is performed which, for differentiable convex functions, consists of taking a strong convex combination of the k points found by the k processors. A more general synchronization step, applicable to convex as well as nonconvex functions, consists of taking the best point found by the k processors or any point that is better. The fundamental result that we establish is that any accumulation point of the parallel algorithm is stationary for the nonconvex case and is a global solution for the convex case. Computational testing on the Thinking Machines CM-5 multiprocessor indicates a speedup of the order of the number of processors employed. }
}

@misc{LLaMA,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Hospitals,
title = {Federated Learning in Medical Imaging: Part II: Methods, Challenges, and Considerations},
journal = {Journal of the American College of Radiology},
volume = {19},
number = {8},
pages = {975-982},
year = {2022},
issn = {1546-1440},
doi = {https://doi.org/10.1016/j.jacr.2022.03.016},
url = {https://www.sciencedirect.com/science/article/pii/S1546144022002812},
author = {Erfan Darzidehkalani and Mohammad Ghasemi-rad and P.M.A. {van Ooijen}},
keywords = {Federated learning, medical imaging, privacy-preserving machine learning},
abstract = {Federated learning is a machine learning method that allows decentralized training of deep neural networks among multiple clients while preserving the privacy of each client’s data. Federated learning is instrumental in medical imaging because of the privacy considerations of medical data. Setting up federated networks in hospitals comes with unique challenges, primarily because medical imaging data and federated learning algorithms each have their own set of distinct characteristics. This article introduces federated learning algorithms in medical imaging and discusses technical challenges and considerations of real-world implementation of them.}
}

@inproceedings{DivPatel3rdOrderBound,
 author = {Dieuleveut, Aymeric and Patel, Kumar Kshitij},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Communication trade-offs for Local-SGD with large step size},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/4aadd661908b181d059a117f02fbc9ec-Paper.pdf},
 volume = {32},
 year = {2019}
}

@misc{Hetero<Id,
      title={Measuring the Effects of Non-Identical Data Distribution for Federated Visual Classification}, 
      author={Tzu-Ming Harry Hsu and Hang Qi and Matthew Brown},
      year={2019},
      eprint={1909.06335},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{Li,
      title={On the Convergence of FedAvg on Non-IID Data}, 
      author={Xiang Li and Kaixuan Huang and Wenhao Yang and Shusen Wang and Zhihua Zhang},
      year={2020},
      eprint={1907.02189},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{LocalSGD2024,
      title={Asynchronous Local-SGD Training for Language Modeling}, 
      author={Bo Liu and Rachita Chhaparia and Arthur Douillard and Satyen Kale and Andrei A. Rusu and Jiajun Shen and Arthur Szlam and Marc'Aurelio Ranzato},
      year={2024},
      eprint={2401.09135},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}